{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa122a7-7420-4b7e-9afc-4a7bbeb5c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5947326-f6a3-4aeb-a194-5cee201d4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym, arg_params, aux_params = mx.model.load_checkpoint(\"cnocr-v1.2.0-densenet-lite-gru\", 39)\n",
    "\n",
    "data_names = ['data']\n",
    "data_shapes = [(data_names[0], (1, 1, 32, 280))]\n",
    "\n",
    "pred_fc = sym.get_internals()['pred_fc_output']\n",
    "sym = mx.sym.softmax(data=pred_fc)\n",
    "\n",
    "context = mx.cpu()\n",
    "\n",
    "mod = mx.mod.Module(\n",
    "    symbol=sym, context=context, data_names=data_names, label_names=None\n",
    ")\n",
    "mod.bind(for_training=False, data_shapes=data_shapes)\n",
    "mod.set_params(arg_params, aux_params, allow_missing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9c7518-cc40-4736-8655-8ed9186195f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet0_stage0_conv0_weight (32, 1, 3, 3)\n",
      "densenet0_stage0_batchnorm0_gamma (32,)\n",
      "densenet0_stage0_batchnorm0_beta (32,)\n",
      "densenet0_stage0_conv1_weight (64, 32, 3, 3)\n",
      "densenet0_batchnorm0_gamma (65,)\n",
      "densenet0_batchnorm0_beta (65,)\n",
      "densenet0_conv0_weight (64, 65, 1, 1)\n",
      "densenet0_stage1_batchnorm0_gamma (64,)\n",
      "densenet0_stage1_batchnorm0_beta (64,)\n",
      "densenet0_stage1_conv0_weight (64, 64, 1, 1)\n",
      "densenet0_stage1_batchnorm1_gamma (64,)\n",
      "densenet0_stage1_batchnorm1_beta (64,)\n",
      "densenet0_stage1_conv1_weight (32, 64, 3, 3)\n",
      "densenet0_stage1_batchnorm2_gamma (96,)\n",
      "densenet0_stage1_batchnorm2_beta (96,)\n",
      "densenet0_stage1_conv2_weight (64, 96, 1, 1)\n",
      "densenet0_stage1_batchnorm3_gamma (64,)\n",
      "densenet0_stage1_batchnorm3_beta (64,)\n",
      "densenet0_stage1_conv3_weight (32, 64, 3, 3)\n",
      "densenet0_batchnorm1_gamma (128,)\n",
      "densenet0_batchnorm1_beta (128,)\n",
      "densenet0_conv1_weight (128, 128, 1, 1)\n",
      "densenet0_stage2_batchnorm0_gamma (128,)\n",
      "densenet0_stage2_batchnorm0_beta (128,)\n",
      "densenet0_stage2_conv0_weight (128, 128, 1, 1)\n",
      "densenet0_stage2_batchnorm1_gamma (128,)\n",
      "densenet0_stage2_batchnorm1_beta (128,)\n",
      "densenet0_stage2_conv1_weight (64, 128, 3, 3)\n",
      "densenet0_stage2_batchnorm2_gamma (192,)\n",
      "densenet0_stage2_batchnorm2_beta (192,)\n",
      "densenet0_stage2_conv2_weight (128, 192, 1, 1)\n",
      "densenet0_stage2_batchnorm3_gamma (128,)\n",
      "densenet0_stage2_batchnorm3_beta (128,)\n",
      "densenet0_stage2_conv3_weight (64, 128, 3, 3)\n",
      "densenet0_last_trans_batchnorm0_gamma (256,)\n",
      "densenet0_last_trans_batchnorm0_beta (256,)\n",
      "densenet0_last_trans_conv0_weight (256, 256, 1, 1)\n",
      "densenet0_last_trans_conv1_weight (256, 1, 2, 3)\n",
      "densenet0_stage3_batchnorm0_gamma (256,)\n",
      "densenet0_stage3_batchnorm0_beta (256,)\n",
      "gru0_l0_i2h_weight (384, 512)\n",
      "gru0_l0_h2h_weight (384, 128)\n",
      "gru0_r0_i2h_weight (384, 512)\n",
      "gru0_r0_h2h_weight (384, 128)\n",
      "gru0_l0_i2h_bias (384,)\n",
      "gru0_l0_h2h_bias (384,)\n",
      "gru0_r0_i2h_bias (384,)\n",
      "gru0_r0_h2h_bias (384,)\n",
      "pred_fc_weight (6426, 256)\n",
      "pred_fc_bias (6426,)\n",
      "densenet0_stage0_batchnorm0_running_mean (32,)\n",
      "densenet0_stage0_batchnorm0_running_var (32,)\n",
      "densenet0_batchnorm0_running_mean (65,)\n",
      "densenet0_batchnorm0_running_var (65,)\n",
      "densenet0_stage1_batchnorm0_running_mean (64,)\n",
      "densenet0_stage1_batchnorm0_running_var (64,)\n",
      "densenet0_stage1_batchnorm1_running_mean (64,)\n",
      "densenet0_stage1_batchnorm1_running_var (64,)\n",
      "densenet0_stage1_batchnorm2_running_mean (96,)\n",
      "densenet0_stage1_batchnorm2_running_var (96,)\n",
      "densenet0_stage1_batchnorm3_running_mean (64,)\n",
      "densenet0_stage1_batchnorm3_running_var (64,)\n",
      "densenet0_batchnorm1_running_mean (128,)\n",
      "densenet0_batchnorm1_running_var (128,)\n",
      "densenet0_stage2_batchnorm0_running_mean (128,)\n",
      "densenet0_stage2_batchnorm0_running_var (128,)\n",
      "densenet0_stage2_batchnorm1_running_mean (128,)\n",
      "densenet0_stage2_batchnorm1_running_var (128,)\n",
      "densenet0_stage2_batchnorm2_running_mean (192,)\n",
      "densenet0_stage2_batchnorm2_running_var (192,)\n",
      "densenet0_stage2_batchnorm3_running_mean (128,)\n",
      "densenet0_stage2_batchnorm3_running_var (128,)\n",
      "densenet0_last_trans_batchnorm0_running_mean (256,)\n",
      "densenet0_last_trans_batchnorm0_running_var (256,)\n",
      "densenet0_stage3_batchnorm0_running_mean (256,)\n",
      "densenet0_stage3_batchnorm0_running_var (256,)\n"
     ]
    }
   ],
   "source": [
    "for key, value in mod.get_params()[0].items():\n",
    "    print(key, value.shape)\n",
    "    \n",
    "for key, value in mod.get_params()[1].items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55979fd9-9427-49e4-9015-8ad9b3210a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17737736-5e9c-4767-9ac0-743b5c6f6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(shape):   # (1, 32, None)  \n",
    "    data = Input(shape=shape)\n",
    "    \n",
    "    x = layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(data)\n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x = layers.Concatenate(axis=1)([data, x])\n",
    "    \n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1), \n",
    "                      data_format=\"channels_first\",  dilation_rate=(1, 1), groups=1, use_bias=False)(x)\n",
    "    x_1 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\", data_format=\"channels_first\")(x)\n",
    "    \n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x_1)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x_1 = layers.Concatenate(axis=1)([x_1, x])\n",
    "    \n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x_1)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x = layers.Concatenate(axis=1)([x_1, x])\n",
    "    \n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=128, kernel_size=(1, 1), strides=(1, 1), \n",
    "                      data_format=\"channels_first\",  dilation_rate=(1, 1), groups=1, use_bias=False)(x)\n",
    "    x_1 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\", data_format=\"channels_first\")(x)\n",
    "    \n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x_1)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=128, kernel_size=(1, 1), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x_1 = layers.Concatenate(axis=1)([x_1, x])\n",
    "    \n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x_1)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=128, kernel_size=(1, 1), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x = layers.Concatenate(axis=1)([x_1, x])\n",
    "    \n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=256, kernel_size=(1, 1), strides=(1, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=1, use_bias=False)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters=256, kernel_size=(2, 3), strides=(2, 1), \n",
    "                      padding=\"same\", data_format=\"channels_first\",  dilation_rate=(1, 1), \n",
    "                      groups=256, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(axis=1, momentum=0.9, epsilon=0.00001)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D(pool_size=(2, 1), strides=(2, 1), padding=\"valid\", data_format=\"channels_first\")(x)\n",
    "    x = layers.Reshape([512, -1])(x)\n",
    "    x = layers.Permute([2, 1])(x)\n",
    "    \n",
    "    x = layers.Bidirectional(layers.GRU(128, return_sequences=True))(x)\n",
    "    x = layers.Dense(6426)(x)\n",
    "    x = layers.Softmax()(x)\n",
    "    \n",
    "    model = Model(data, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1edcd6f3-6511-4842-9652-7e94f98ade1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model((1, 32, None))\n",
    "model.build((1, 32, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c8e4ca-fc15-4ecc-8b1a-fd127d8f5f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d/kernel:0 (3, 3, 1, 32)\n",
      "batch_normalization/gamma:0 (32,)\n",
      "batch_normalization/beta:0 (32,)\n",
      "batch_normalization/moving_mean:0 (32,)\n",
      "batch_normalization/moving_variance:0 (32,)\n",
      "conv2d_1/kernel:0 (3, 3, 32, 64)\n",
      "batch_normalization_1/gamma:0 (65,)\n",
      "batch_normalization_1/beta:0 (65,)\n",
      "batch_normalization_1/moving_mean:0 (65,)\n",
      "batch_normalization_1/moving_variance:0 (65,)\n",
      "conv2d_2/kernel:0 (1, 1, 65, 64)\n",
      "batch_normalization_2/gamma:0 (64,)\n",
      "batch_normalization_2/beta:0 (64,)\n",
      "batch_normalization_2/moving_mean:0 (64,)\n",
      "batch_normalization_2/moving_variance:0 (64,)\n",
      "conv2d_3/kernel:0 (1, 1, 64, 64)\n",
      "batch_normalization_3/gamma:0 (64,)\n",
      "batch_normalization_3/beta:0 (64,)\n",
      "batch_normalization_3/moving_mean:0 (64,)\n",
      "batch_normalization_3/moving_variance:0 (64,)\n",
      "conv2d_4/kernel:0 (3, 3, 64, 32)\n",
      "batch_normalization_4/gamma:0 (96,)\n",
      "batch_normalization_4/beta:0 (96,)\n",
      "batch_normalization_4/moving_mean:0 (96,)\n",
      "batch_normalization_4/moving_variance:0 (96,)\n",
      "conv2d_5/kernel:0 (1, 1, 96, 64)\n",
      "batch_normalization_5/gamma:0 (64,)\n",
      "batch_normalization_5/beta:0 (64,)\n",
      "batch_normalization_5/moving_mean:0 (64,)\n",
      "batch_normalization_5/moving_variance:0 (64,)\n",
      "conv2d_6/kernel:0 (3, 3, 64, 32)\n",
      "batch_normalization_6/gamma:0 (128,)\n",
      "batch_normalization_6/beta:0 (128,)\n",
      "batch_normalization_6/moving_mean:0 (128,)\n",
      "batch_normalization_6/moving_variance:0 (128,)\n",
      "conv2d_7/kernel:0 (1, 1, 128, 128)\n",
      "batch_normalization_7/gamma:0 (128,)\n",
      "batch_normalization_7/beta:0 (128,)\n",
      "batch_normalization_7/moving_mean:0 (128,)\n",
      "batch_normalization_7/moving_variance:0 (128,)\n",
      "conv2d_8/kernel:0 (1, 1, 128, 128)\n",
      "batch_normalization_8/gamma:0 (128,)\n",
      "batch_normalization_8/beta:0 (128,)\n",
      "batch_normalization_8/moving_mean:0 (128,)\n",
      "batch_normalization_8/moving_variance:0 (128,)\n",
      "conv2d_9/kernel:0 (3, 3, 128, 64)\n",
      "batch_normalization_9/gamma:0 (192,)\n",
      "batch_normalization_9/beta:0 (192,)\n",
      "batch_normalization_9/moving_mean:0 (192,)\n",
      "batch_normalization_9/moving_variance:0 (192,)\n",
      "conv2d_10/kernel:0 (1, 1, 192, 128)\n",
      "batch_normalization_10/gamma:0 (128,)\n",
      "batch_normalization_10/beta:0 (128,)\n",
      "batch_normalization_10/moving_mean:0 (128,)\n",
      "batch_normalization_10/moving_variance:0 (128,)\n",
      "conv2d_11/kernel:0 (3, 3, 128, 64)\n",
      "batch_normalization_11/gamma:0 (256,)\n",
      "batch_normalization_11/beta:0 (256,)\n",
      "batch_normalization_11/moving_mean:0 (256,)\n",
      "batch_normalization_11/moving_variance:0 (256,)\n",
      "conv2d_12/kernel:0 (1, 1, 256, 256)\n",
      "conv2d_13/kernel:0 (2, 3, 1, 256)\n",
      "batch_normalization_12/gamma:0 (256,)\n",
      "batch_normalization_12/beta:0 (256,)\n",
      "batch_normalization_12/moving_mean:0 (256,)\n",
      "batch_normalization_12/moving_variance:0 (256,)\n",
      "bidirectional/forward_gru/gru_cell_1/kernel:0 (512, 384)\n",
      "bidirectional/forward_gru/gru_cell_1/recurrent_kernel:0 (128, 384)\n",
      "bidirectional/forward_gru/gru_cell_1/bias:0 (2, 384)\n",
      "bidirectional/backward_gru/gru_cell_2/kernel:0 (512, 384)\n",
      "bidirectional/backward_gru/gru_cell_2/recurrent_kernel:0 (128, 384)\n",
      "bidirectional/backward_gru/gru_cell_2/bias:0 (2, 384)\n",
      "dense/kernel:0 (256, 6426)\n",
      "dense/bias:0 (6426,)\n"
     ]
    }
   ],
   "source": [
    "for each in model.weights:\n",
    "    print(each.name, each.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7245a11-b46e-444e-9ca2-e212e9a789ed",
   "metadata": {},
   "source": [
    "### 验证输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1791855-1e14-4389-bd0e-e3d3259b618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76e2685-49c8-45a3-9cd7-5df44f92b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((16, 1, 32, 280)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9be149-9fa8-4b11-87c6-e242c7b1979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transport_each(mx_layer_name, mx_model):\n",
    "    if \"conv\" in mx_layer_name:\n",
    "        mx_weight = mx_model.get_params()[0][mx_layer_name + \"_weight\"].asnumpy()\n",
    "        tf_weights = [tf.transpose(mx_weight, perm=[2, 3, 1, 0])]\n",
    "    elif \"batchnorm\" in mx_layer_name:\n",
    "        mx_gamma = mx_model.get_params()[0][mx_layer_name + \"_gamma\"].asnumpy()\n",
    "        mx_beta = mx_model.get_params()[0][mx_layer_name + \"_beta\"].asnumpy()\n",
    "        mx_running_mean = mx_model.get_params()[1][mx_layer_name + \"_running_mean\"].asnumpy()\n",
    "        mx_running_var = mx_model.get_params()[1][mx_layer_name + \"_running_var\"].asnumpy()\n",
    "        \n",
    "        tf_weights = [mx_gamma, mx_beta, mx_running_mean, mx_running_var]\n",
    "    elif \"gru\" in mx_layer_name and \"i2h\" in mx_layer_name:\n",
    "        mx_layer_name = mx_layer_name[:-4]\n",
    "        mx_i2h_weight = mx_model.get_params()[0][mx_layer_name + \"_i2h_weight\"].asnumpy()\n",
    "        mx_h2h_weight = mx_model.get_params()[0][mx_layer_name + \"_h2h_weight\"].asnumpy()\n",
    "        mx_i2h_bias = mx_model.get_params()[0][mx_layer_name + \"_i2h_bias\"].asnumpy()\n",
    "        mx_h2h_bias = mx_model.get_params()[0][mx_layer_name + \"_h2h_bias\"].asnumpy()\n",
    "        \n",
    "        mx_i2h_weight = tf.transpose(mx_i2h_weight, [1, 0])   # (512, 384)\n",
    "        mx_h2h_weight = tf.transpose(mx_h2h_weight, [1, 0])   # (128, 384)\n",
    "        \n",
    "        Wr, Wz, Wh = tf.split(mx_i2h_weight, 3, axis=1)   # (512, 128)\n",
    "        Rr, Rz, Rh = tf.split(mx_h2h_weight, 3, axis=1)\n",
    "        Wbr, Wbz, Wbh = tf.split(mx_i2h_bias, 3, axis=0)   # (128,)\n",
    "        Rbr, Rbz, Rbh = tf.split(mx_h2h_bias, 3, axis=0)\n",
    "        \n",
    "        tf_kernel = tf.concat([Wz, Wr, Wh], axis=1)\n",
    "        tf_recurrent_kernel = tf.concat([Rz, Rr, Rh], axis=1)\n",
    "        tf_bias = tf.stack([tf.concat([Wbz, Wbr, Wbh], axis=0), tf.concat([Rbz, Rbr, Rbh], axis=0)], axis=0)\n",
    "        \n",
    "        tf_weights = [tf_kernel, tf_recurrent_kernel, tf_bias]\n",
    "    elif \"pred_fc\" in mx_layer_name:\n",
    "        mx_weight = mx_model.get_params()[0][mx_layer_name + \"_weight\"].asnumpy()\n",
    "        mx_bias = mx_model.get_params()[0][mx_layer_name + \"_bias\"].asnumpy()\n",
    "        \n",
    "        tf_weight = tf.transpose(mx_weight, [1, 0])\n",
    "        \n",
    "        tf_weights = [tf_weight, mx_bias]\n",
    "    else:\n",
    "        tf_weights = []\n",
    "    \n",
    "    return tf_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "773b902b-fc10-4813-a6bf-15a5a8c770c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(a):\n",
    "    b = list(set(a))\n",
    "    b.sort(key=a.index)\n",
    "    \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a766b12-8f04-4ac1-9c9d-3042f8f08176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transport_weights(mx_model):\n",
    "    mx_layers = list(map(lambda x: \"_\".join(x.split(\"_\")[:-1]), mx_model.get_params()[0].keys()))\n",
    "    mx_layers = dedup(mx_layers)\n",
    "    \n",
    "    # tf_layers = [each.name.split(\"/\")[0] for each in tf_model.weights]\n",
    "    # tf_layers = dedup(tf_layers)\n",
    "    \n",
    "    tf_weights = list()\n",
    "    \n",
    "    for layer in mx_layers:\n",
    "        weights = transport_each(layer, mx_model)\n",
    "        tf_weights.extend(weights)\n",
    "        \n",
    "    return tf_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "485e21ff-0d06-496a-910b-9ad8d7f89331",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(transport_weights(mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2171f6ec-9d8d-41d5-8662-54f5c3bc8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95fb6adc-592a-4df0-915b-9d5d4ad4dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = tf.transpose(y_hat, [1, 0, 2])\n",
    "y_hat = tf.reshape(y_hat, (-1, 6426))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "676b2ced-b6f3-4581-83a7-74633ac8bf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1120, 6426])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "896b4f5c-4343-4d18-bfb2-69577ed8ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_test, arg_params_test, aux_params_test = mx.model.load_checkpoint(\"cnocr-v1.2.0-densenet-lite-gru\", 39)\n",
    "\n",
    "data_names_test = ['data']\n",
    "data_shapes_test = [(data_names_test[0], (16, 1, 32, 280))]\n",
    "context_test = mx.cpu()\n",
    "\n",
    "# pool0_fwd_output_test = sym_test.get_internals()['gru0_rnn0_output']\n",
    "pool0_fwd_output_test = sym_test.get_internals()['softmaxactivation0_output']\n",
    "\n",
    "mod_test = mx.mod.Module(\n",
    "    symbol=pool0_fwd_output_test, context=context_test, data_names=data_names_test, label_names=None\n",
    ")\n",
    "mod_test.bind(for_training=False, data_shapes=data_shapes_test)\n",
    "mod_test.set_params(arg_params_test, aux_params_test, allow_missing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da7c2f64-d90b-481f-ab70-86550ff826d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = namedtuple(\"Batch\", [\"data\"])\n",
    "mod_test.forward(Batch([mx.nd.array(data)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6914412b-317e-4bfd-a8b5-6f3bcf8f0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_mx = mod_test.get_outputs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80aa3552-0144-4265-b2ef-4700dc1003b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120, 6426)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_mx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b57b102e-78df-4b4f-88b0-fd9d1f0e8c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.54647636e-06, -6.60656951e-09, -8.68567440e-11, ...,\n",
       "        -1.48361323e-11, -1.06226139e-12, -3.46944695e-16],\n",
       "       [-1.81794167e-05, -1.25055521e-11, -1.12132525e-13, ...,\n",
       "        -6.22765728e-16, -1.33226763e-13, -5.09141340e-16],\n",
       "       [-3.69548798e-05, -6.98491931e-10, -2.06057393e-12, ...,\n",
       "        -5.57065505e-12, -7.44293516e-13, -1.87566976e-17],\n",
       "       ...,\n",
       "       [-1.31130219e-06, -6.48786580e-16, -8.67361738e-18, ...,\n",
       "        -3.07642366e-18, -1.98729921e-14, -4.04190570e-16],\n",
       "       [-8.34465027e-07, -3.60822483e-16,  1.12323345e-16, ...,\n",
       "         1.21972744e-18, -1.38777878e-16,  5.69206141e-19],\n",
       "       [-4.76837158e-07, -2.45463372e-16, -2.03287907e-18, ...,\n",
       "        -1.02999206e-18, -1.38777878e-16, -4.22838847e-18]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_hat) - y_hat_mx.asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20af1574-2794-4806-9921-999b7f703419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"densenet-lite-gru.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b77c3-c74a-4a9f-9ca9-6ad1954b65cc",
   "metadata": {},
   "source": [
    "### 使用样本测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9e6dca4-d210-43a8-8556-6a32d38dbd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnocr.fit.ctc_metrics import CtcMetrics\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0e8605b-c29e-4159-8af1-1cc415650420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_charset(charset_fp):\n",
    "    alphabet = [None]\n",
    "    # 第0个元素是预留id，在CTC中用来分割字符。它不对应有意义的字符\n",
    "    with open(charset_fp, encoding='utf-8') as fp:\n",
    "        for line in fp:\n",
    "            alphabet.append(line.rstrip('\\n'))\n",
    "    # print('Alphabet size: %d' % len(alphabet))\n",
    "    try:\n",
    "        space_idx = alphabet.index('<space>')\n",
    "        alphabet[space_idx] = ' '\n",
    "    except ValueError:\n",
    "        pass\n",
    "    inv_alph_dict = {_char: idx for idx, _char in enumerate(alphabet)}\n",
    "    \n",
    "    return alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a384d24-56d5-40e9-8b5e-2b0ad6de9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_line_pred_chars(line_prob, img_width, max_img_width, alphabet):\n",
    "    \"\"\"\n",
    "    Get the predicted characters.\n",
    "    :param line_prob: with shape of [seq_length, num_classes]\n",
    "    :param img_width:\n",
    "    :param max_img_width:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    class_ids = np.argmax(line_prob, axis=-1)\n",
    "\n",
    "    if img_width < max_img_width:\n",
    "        comp_ratio = 8\n",
    "        end_idx = img_width // comp_ratio\n",
    "        if end_idx < len(class_ids):\n",
    "            class_ids[end_idx:] = 0\n",
    "    prediction, start_end_idx = CtcMetrics.ctc_label(class_ids.tolist())\n",
    "\n",
    "    res = [alphabet[p] if alphabet[p] != '<space>' else ' ' for p in prediction]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec5ad962-1079-4111-81e3-7e44cc9e5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = read_charset(\"label_cn.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a8dcdec-4f73-4875-8be9-f345ec4efdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = \"../zzb0001.jpg\"\n",
    "test_img = cv2.imread(test_img_path, 0)\n",
    "height, width = test_img.shape\n",
    "rate = 32 / height\n",
    "width *= rate\n",
    "test_img = cv2.resize(test_img, (int(width), 32))\n",
    "test_img = np.expand_dims(test_img, axis=0)\n",
    "test_img = np.expand_dims(test_img, axis=0)\n",
    "test_img = test_img.astype(np.float32)\n",
    "test_img /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8fe68fc-9e2e-48b2-9ad6-e0b0bf4a505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_prob = model(test_img)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c28b0ba-f34c-43a1-8f8f-0e1a8450b626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'山西云通软件省管干部年度考核测评表'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(gen_line_pred_chars(line_prob, width, width, alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb059d-6f8c-4fad-bff3-12b0d9d7bb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
